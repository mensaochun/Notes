

# PRML notes

latex公式参考：http://blog.csdn.net/fansongy/article/details/45368915

## 1.绪论

###1.1例子：多项式拟合

####最小平方方法

1. 最小平方方法存在过拟合的问题→可以通过增大数据的数量和增加正则项来解决。
2. 模型复杂度的选择可以通过交叉验证方法得到。
3. 实际上，之后可以看到，寻找模型参数的最小平方方法代表了极大似然，线性回归的例子。
4. 贝叶斯方法中，参数的数量会根据数据集自动调整，以此解决过拟合的问题。

### 1.2 概率论

通过沙子方格的例子，推导出概率论的基本规则：求和规则和乘积规则。

求和规则，实际上就是求边缘分布。实际上可以看成是投影。

乘积规则，实际上就是求联合概率。

条件概率：实际上是缩小了样本空间的概率。

注意联想到沙子方格就没有问题了。

需要掌握的知识：

2.通过沙子方格的例子，得到加和规则和乘积规则。

注意：条件概率的和为1.p(x/y)，在Y=y的时候，sum(p(X=xi/Y=y))为1

还有贝叶斯定理。

1.通过推导概率的基本规则：加和规则和乘积规则。例子：一个红色盒子，里面2个苹果和6个橘子。一个蓝色盒子，里面有3个苹果和1个橘子。假设我们现在在40%的时间里选择了红色盒子，在60%的时间里选择蓝色的盒子。

问题：A.请问选到苹果的概率是多少？B.选择了橘子，name选择的盒子是蓝色盒子的概率是多少？

问题A解答：首先可以设选到盒子的颜色为随机变量B，选到水果的种类为随机变量F。由此可知p(B=r)=0.4, p(B=b)=0.6，问题转化为求解p(F=a)。







1.5.4 inference and decision

 

分类问题可以分成两部分：推断和决策。推断就是学习到一个后验分布：![img](file:////tmp/wps-pi/ksohtml/wpsYEr9Tx.jpg)。

决策就是根据这个后验概率该出一个类别的最优安排。

 

实际上，有三种决策方法，按照复杂度递减的排列可以分为判别函数（discriminate function）

 

A.生成模型：为每个类别Ck 各自计算出一个类条件概率和先验概率P(Ck)，再根据贝叶斯公式计算出后验概率。当然我们也可以显式或者隐式地计算出联合概率分布P(x,Ck),然后得到以上的概率。这种称为生成模型。

 

B.判别模型：直接得到后验类别概率，![img](file:////tmp/wps-pi/ksohtml/wpsD7e6sQ.jpg)然后由这个后验概率经过决策理论将新的输入x映射到类别。

![img](file:////tmp/wps-pi/ksohtml/wpsaGhb28.jpg) 

C.判别函数：直接找一个函数，讲输入x直接映射到类别。

 

A方法的优缺点：

生成模型的条件非常苛刻，因为它需要寻找到x，Ck的联合分布，当x的维度很高的时候，这时候就需要非常大量的数据来得到联合概率分布。但是它有个优点就是：![img](file:////tmp/wps-pi/ksohtml/wpsyyPmBr.jpg)

可以得到x的概率分布，当一个新的数据x得到的概率p(x)非常低的时候可以被当做异常，这个就是广为人知的异常检测。

 

然而，类条件概率密度可能尽管有很多复杂的结构，但是对决策来讲可能影响并没有那么大，也就是说，我们可能并不需要知道得那么多也可以做好决策。

 

C能够直接计算出x属于哪个类别，但是这个过程中忽略了计算后验概率![img](file:////tmp/wps-pi/ksohtml/wps8aXAaK.jpg)后验概率还是有很多好处的，且看以下三点好处：

 

计算出后验概率有一些好处

 

 ## 第三章







**第四章******

** **

第四章

 

对于分类问题，有三种解决思路。

最简单的一种，就是判别函数，直接将x分到某个类。

第二种，在推断阶段计算p(ck/x)，在决策阶段再将x分到最优的那一类。得到p(ck/x)又有两种方法：1.判别模型：直接建立一个包含参数的条件概率模型p(ck/x)。2.生成模型，通过计算先验p（ck）和类条件概率p(x/ck)，再通过贝叶斯方法得到后验概率p(ck/x)。问题，p(x)呢？

 

广义线性模型

f(wx+b), f可以是线性函数，也可以是非线性函数，总之，这些都称为广义线性模型。

 

4.1 判别函数

第四章

 

对于分类问题，有三种解决思路。

最简单的一种，就是判别函数，直接将x分到某个类。

第二种，在推断阶段计算p(ck/x)，在决策阶段再将x分到最优的那一类。得到p(ck/x)又有两种方法：1.判别模型：直接建立一个包含参数的条件概率模型p(ck/x)。2.生成模型，通过计算先验p（ck）和类条件概率p(x/ck)，再通过贝叶斯方法得到后验概率p(ck/x)。问题，p(x)呢？

 

广义线性模型

f(wx+b), f可以是线性函数，也可以是非线性函数，总之，这些都称为广义线性模型。

** **

** **

4.1 .1判别函数

权重决定决策面的旋转方向，偏置决定决策面和远点的距离。

4.1.2多分类

 

one-verus-the-rest方法的问题：出现无法分类的区域。

one-versus-one ：构建K(K-1)个二分类器。决策的时候进行投票。投票最多的获胜。问题：也会出现模糊地带。

 

解决方法：构建包括k个线性函数，决策的时候选择一个最大的就好了。可以证明得到的这些分类区域都是单联通，凸的。

 

Note:对于二分类问题。可以构建一个二分类器，也可以构建2个判别函数，这两种形式都是可以的。

 

4.1.3 最小平方方法来做分类

实际上是近似E(t/x)。

第四章

 

对于分类问题，有三种解决思路。

最简单的一种，就是判别函数，直接将x分到某个类。

第二种，在推断阶段计算p(ck/x)，在决策阶段再将x分到最优的那一类。得到p(ck/x)又有两种方法：1.判别模型：直接建立一个包含参数的条件概率模型p(ck/x)。2.生成模型，通过计算先验p（ck）和类条件概率p(x/ck)，再通过贝叶斯方法得到后验概率p(ck/x)。问题，p(x)呢？

 

广义线性模型

f(wx+b), f可以是线性函数，也可以是非线性函数，总之，这些都称为广义线性模型。

 

4.1 判别函数

 

 

fisher线性判别函数

线性分类器可以看做是一种降维。y=WTx,如果y>w0则判别为正类，否则判别为负类。这种降维可能导致的一个问题就是类别在单一维度上不容易区分，但是通过调整w可以寻找到一个最合适的投影，让类别之间足以分开。

fisher分类器的思想：对于二分类问题，将每一个类的samples进行相加，分别得到均值向量，让这两个均值向量的差最大化。

存在的问题：投影之后，在1D空间中，类别之间会出现重叠现象。解决的方法，使类之类的方差最小化。

 

综合两者，得到fisher判别函数的准则：1.使类间均值差最大，2.让类内方差尽量小。通过优化即可以得到fisher线性判别器。

 

感知机算法

 

感知机算法就是为了寻找误分类数目最小

 

 

概率生成模型

 

思想：计算类条件概率p(x/ck)和类先验p（ck），然后由此得到后验概率p（ck）

可以分成几个步骤来做：

1.由贝叶斯公式得到后验概率p（ck）的形式

![img](file:////tmp/wps-pi/ksohtml/wps0WsaK2.png)2.假设类条件概率分布的形式

比如说，假设类条件概率为高斯：p(x/ck)，且假设每个类之间共享协方差矩阵，但是均值各自不同。

3. 由类条件概率和先验，得到后验概率。



![img](file:////tmp/wps-pi/ksohtml/wpsXXF0jl.png)

 

 

 

 

 

 

 

 

 

 

 

 

4.极大似然进行参数估计

 

类条件概率p(x/ck)中存在一些未知的参数需要求解，比如共享的协方差矩阵sigma，比如与类有关的均值uk。这些参数可以通过极大似然方法求得。

首先计算联合概率分布p(x，c1)和p(x，c2)

![img](file:////tmp/wps-pi/ksohtml/wpsScK1TD.png)

 

然后计算似然函数（特别要注意的是，似然函数可以是联合概率分布的乘积）：

![img](file:////tmp/wps-pi/ksohtml/wpsP7T9tW.png)

然后通过极大化似然函数求得参数值，再带入2中，就可以求得后验分布。

 

另外，需要特别注意，假定类条件概率分布为高斯分布有一个问题：对outliers是很敏感的。

 

 

概率判别模型

 

概率生成模型之所以这么称呼，是因为可以得到概率分布p（x），由此可以生成数据。

生成模型参数更多，比如以上的情况中，假设特征的维度为D，那么u1，u2和协方差矩阵和类先验，这些参数总和就非常多，判别模型的参数则与特征成线性关系，就为D。

 

logistic回归

逻辑回归直接假设了后验概率的形式为sigmoid函数，即...

得到似然函数

![img](file:////tmp/wps-pi/ksohtml/wpscFUp4e.png)

 

 

 

 

 

 

 

 

通过极大化似然函数，可以得到解。其中y的形式是sigmoid函数。

 

 ## 第六章 核方法

### 1.两种模型

一种模型，用训练数据训练出模型的参数，在预测阶段舍弃训练数据，只涉及模型参数。

另一种模型，在预测阶段仍旧需要训练数据。这种情况又可以分为两种，一种是预测时候需要全部训练数据，比如：最邻近算法。另一种是预测时只需要部分的训练数据，比如svm，只需要支持向量。



## 2.Kernal

核函数由非线性特征空间映射得到：



注意上式中，是一个向量，里面包含了很多基函数，当是恒等映射的时候，则称为线性核。如果我们有⼀个算法，它的输⼊向量x只以标量积的形式出现，那么我们可以⽤⼀些其他
的核来替换这个标量积 。



对线性模型进行



## 对偶表示的一个例子

很多回归的线性模型核分类的线性模型都可以用对偶表示来重写（也就是用核函数的形式表表示？），通过一个例子来理解。考虑一个带有正则化的线性模型，误差函数如下：

可以通过梯度下降的方法来解得参数，也可以通过令误差函数的导数为0来得到参数的解析解。通过改造，最终得到以下预测模型：



改造后的缺点：通过NxN的矩阵求逆，这个计算量通常会很大。原始求解是对MxM的矩阵求逆。

优点：可以完全通过核函数来表示，这样避免了显示定义特征映射函数，而可以隐式地映射到无穷维。



## 构造核

构造核函数的几种方法：

1. 显示地指定特征空间映射函数，然后根据这个函数得到相应的核函数。
2. 直接构造核函数。但是，核函数必须满足一定的条件才行。核函数是一个合法的核函数的充分必要条件是：Gram矩阵（元素由k(xn; xm)给出）在所有的集合fxng的选择下都是半正定的。
3. 从一个概率生成式模型开始构造。



### 径向基函数网络



径向基函数：每⼀个基函数只依赖于样本和中⼼$$µ_j$$之间的径向距离（通常是欧⼏⾥得距离），即$$ϕ_j(x) = h(∥x - µ_j∥) $$

> 插曲：径向基函数可以用来做函数内插。
>
>    

https://www.zhihu.com/question/46631426